{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from data_process.data_processor import DataProcessor\n",
    "from nuplan.common.actor_state.state_representation import Point2D\n",
    "from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_utils import ScenarioMapping\n",
    "from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_builder import NuPlanScenarioBuilder\n",
    "from nuplan.planning.scenario_builder.scenario_filter import ScenarioFilter\n",
    "from nuplan.planning.utils.multithreading.worker_parallel import SingleMachineParallelExecutor\n",
    "from diffusion_planner.data_process.utils import (\n",
    "get_scenario_map,\n",
    "get_filter_parameters,\n",
    "sampled_tracked_objects_to_tensor,\n",
    ")\n",
    "\n",
    "from diffusion_planner.data_process.roadblock_utils import (\n",
    "  route_roadblock_correction\n",
    ")\n",
    "\n",
    "\n",
    "from diffusion_planner.data_process.agent_process import(sampled_tracked_objects_to_array_list,\n",
    "                                                         sampled_static_objects_to_array_list,\n",
    "                                                         agent_past_process,)\n",
    "from diffusion_planner.data_process.map_process import(map_process,\n",
    "                                                       get_neighbor_vector_set_map)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker threads: 128\n"
     ]
    }
   ],
   "source": [
    "db_path = \"/cailiu2/Diffusion-Planner/data/2021.10.21.14.43.30_veh-28_01244_01519.db\" # single db file\n",
    "db_path = \"/share/data_cold/open_data/nuplan/data/cache/mini\"  # mini db files\n",
    "\n",
    "map_path = \"/share/data_cold/open_data/nuplan/maps\"\n",
    "save_path = \"/cailiu2/Diffusion-Planner/data/processed\"\n",
    "map_version = \"nuplan-maps-v1.0\"\n",
    "\n",
    "save_processed_path = \"/cailiu2/Diffusion-Planner/data/processed\"\n",
    "total_scenarios = 10\n",
    "device = torch.device(\"cuda:0\")\n",
    "scenario_mapping = ScenarioMapping(scenario_map=get_scenario_map(), subsample_ratio_override=0.5)\n",
    "builder = NuPlanScenarioBuilder(db_path, map_path, None, None, map_version, scenario_mapping = scenario_mapping)\n",
    "worker = SingleMachineParallelExecutor(use_process_pool=True)\n",
    "scenario_filter = ScenarioFilter(*get_filter_parameters(num_scenarios_per_type=30000,\n",
    "                                                          limit_total_scenarios=total_scenarios))\n",
    "scenarios = builder.get_scenarios(scenario_filter, worker)\n",
    "del worker, builder, scenario_filter\n",
    "processor = DataProcessor(scenarios, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_planner.data_process.utils import (\n",
    "convert_absolute_quantities_to_relative,\n",
    "_global_state_se2_array_to_local,\n",
    "_global_velocity_to_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nuplan.common.actor_state.ego_state.EgoState object at 0x7fc9c47a3e80>\n",
      "(200, 7)\n"
     ]
    }
   ],
   "source": [
    "from nuplan.planning.training.preprocessing.utils.agents_preprocessing import(sampled_past_ego_states_to_tensor)\n",
    "\"\"\"\n",
    "ego past\n",
    "\"\"\"\n",
    "processor.scenario = scenarios[0]\n",
    "print(processor.scenario.initial_ego_state)\n",
    "processor.anchor_ego_state = np.array([processor.scenario.initial_ego_state.rear_axle.x, processor.scenario.initial_ego_state.rear_axle.y, processor.scenario.initial_ego_state.rear_axle.heading], \\\n",
    "                                      dtype=np.float64)\n",
    "\n",
    "past_ego_states = processor.scenario.get_ego_past_trajectory(\n",
    "    iteration=0, num_samples=processor.num_past_poses, time_horizon=processor.past_time_horizon\n",
    ")\n",
    "future_scenario_horizon = processor.scenario.duration_s.time_s - processor.past_time_horizon\n",
    "scenario_num_samples = int(10 * future_scenario_horizon)\n",
    "future_ego_states = processor.scenario.get_ego_future_trajectory(iteration=0, num_samples=scenario_num_samples, time_horizon= future_scenario_horizon)\n",
    "\n",
    "sampled_ego_states = list(past_ego_states) + [processor.scenario.initial_ego_state] + list(future_ego_states)\n",
    "\n",
    "# print(type(sampled_ego_states))\n",
    "# print(len(sampled_ego_states))\n",
    "sampled_ego_states_numpy = []\n",
    "sampled_ego_states_numpy = np.array([(ego_state.rear_axle.x, ego_state.rear_axle.y, ego_state.rear_axle.heading, \\\n",
    "                                      ego_state.dynamic_car_state.rear_axle_velocity_2d.x, ego_state.dynamic_car_state.rear_axle_velocity_2d.y,\\\n",
    "                                      ego_state.dynamic_car_state.rear_axle_acceleration_2d.x, ego_state.dynamic_car_state.rear_axle_acceleration_2d.y ) for ego_state in sampled_ego_states], dtype = np.float64)\n",
    "# print(sampled_ego_states_numpy.shape)\n",
    "\n",
    "sampled_ego_states_numpy = np.pad(sampled_ego_states_numpy, ((0, processor.num_pred_poses), (0,0)), mode = 'constant', constant_values=np.nan)\n",
    "# print(sampled_ego_states_numpy.shape)\n",
    "local_sampled_past_ego_states = convert_absolute_quantities_to_relative(sampled_ego_states_numpy,processor.anchor_ego_state,'ego' )\n",
    "print(local_sampled_past_ego_states.shape)\n",
    "# processor.scenario = scenarios[0]\n",
    "# \"\"\"\n",
    "# ego future\n",
    "# \"\"\"\n",
    "# print(processor.scenario.duration_s)\n",
    "\n",
    "# ego_state = processor.scenario.initial_ego_state\n",
    "# anchor_ego_state = np.array([ego_state.rear_axle.x, ego_state.rear_axle.y, ego_state.rear_axle.heading], dtype=np.float64)\n",
    "# ego_states_cat_numpy = ego_states_cat.numpy()\n",
    "# print(ego_states_cat_numpy.shape)\n",
    "# ego_state_cat_convert = convert_absolute_quantities_to_relative(ego_states_cat_numpy,anchor_ego_state,'ego' )\n",
    "\n",
    "# print(type(ego_state_cat_convert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_tracked_objects(trakced_objects_in_all_frames, full_time_horizon, filter_agent_frames_threshold):\n",
    "\n",
    "#   # 创建一个字典来存储所有障碍物的轨迹\n",
    "#   object_trajectories = {}\n",
    "\n",
    "#   # 遍历每一帧的 tracked_objects\n",
    "#   for frame_idx, detection_frame in enumerate(trakced_objects_in_all_frames):\n",
    "#     for obj in detection_frame.tracked_objects:\n",
    "#         obj_token = obj.track_token  # 获取障碍物 ID\n",
    "#         obj_state = np.array([obj.center.x, obj.center.y, obj.center.heading])  # 获取障碍物位置 (x, y)\n",
    "\n",
    "#         # 如果该障碍物 ID 还未存储，则初始化\n",
    "#         if obj_token not in object_trajectories:\n",
    "#             object_trajectories[obj_token] = []\n",
    "\n",
    "#         # 记录该障碍物在该帧的位置信息\n",
    "#         object_trajectories[obj_token].append(obj_state)\n",
    "\n",
    "#   keys_to_remove = [obj_token for obj_token, traj in object_trajectories.items() if len(traj) < filter_agent_frames_threshold]\n",
    "\n",
    "#   for key in keys_to_remove:\n",
    "#     del object_trajectories[key] #过滤少于filter_agent_frames_threshold帧的障碍物\n",
    "\n",
    "#   # 统一轨迹长度，填充 NaN\n",
    "#   for obj_token, traj in object_trajectories.items():\n",
    "#     traj = np.array(traj)  # 转换为 NumPy 数组，形状为 (N, 2)\n",
    "#     num_frames = traj.shape[0]\n",
    "\n",
    "#     if num_frames < full_time_horizon:\n",
    "#         # 如果轨迹长度小于 V，则用 NaN 填充\n",
    "#         pad_size = full_time_horizon - num_frames\n",
    "#         pad_array = np.full((pad_size, 3), np.nan)  # 生成 (pad_size, 2) 的 NaN 数组\n",
    "#         object_trajectories[obj_token] =np.concatenate([traj, pad_array], axis=0) # 拼接填充\n",
    "#     else:\n",
    "#         # 如果轨迹长度大于 V，则裁剪\n",
    "#         object_trajectories[obj_token] = traj[:full_time_horizon]\n",
    "\n",
    "\n",
    "\n",
    "#   return object_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tracked_objects(tracked_objects_in_all_frames, full_time_horizon, filter_agent_frames_threshold):\n",
    "    \"\"\"\n",
    "    处理自动驾驶中的障碍物数据，将其转换为 (T, N, 3) 形状的 NumPy 数组\n",
    "    :param tracked_objects_in_all_frames: 包含所有时间步的障碍物检测信息\n",
    "    :param full_time_horizon: 需要对齐的时间长度（T）\n",
    "    :param filter_agent_frames_threshold: 过滤掉轨迹长度小于该值的障碍物\n",
    "    :return: 形状为 (T, N, 3) 的 NumPy 数组，N 为所有时间步中最大障碍物数\n",
    "    \"\"\"\n",
    "\n",
    "    # 创建一个字典存储每个障碍物的轨迹\n",
    "    object_trajectories = {}\n",
    "\n",
    "    # 遍历每一帧的 tracked_objects\n",
    "    for frame_idx, detection_frame in enumerate(tracked_objects_in_all_frames):\n",
    "        for obj in detection_frame.tracked_objects:\n",
    "            obj_token = obj.track_token  # 获取障碍物唯一 ID\n",
    "            obj_state = np.array([obj.center.x, obj.center.y, obj.center.heading])  # 提取 (x, y, heading)\n",
    "\n",
    "            # 如果该障碍物 ID 还未存储，则初始化\n",
    "            if obj_token not in object_trajectories:\n",
    "                object_trajectories[obj_token] = np.full((full_time_horizon, 3), np.nan)\n",
    "\n",
    "            # 记录该障碍物在该帧的位置信息\n",
    "            object_trajectories[obj_token][frame_idx] = obj_state\n",
    "\n",
    "    # **第一步：过滤掉轨迹长度小于 filter_agent_frames_threshold 的障碍物**\n",
    "    object_trajectories = {\n",
    "        obj_token: traj for obj_token, traj in object_trajectories.items()\n",
    "        if np.sum(~np.isnan(traj[:, 0])) >= filter_agent_frames_threshold\n",
    "    }\n",
    "\n",
    "    return object_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "150\n",
      "103\n",
      "<class 'numpy.ndarray'>\n",
      "(103, 201, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "agent future\n",
    "\"\"\"\n",
    "filter_agent_frames_threshold = 10\n",
    "neighbors = processor.scenario.initial_tracked_objects\n",
    "neighbor_past = list(processor.scenario.get_past_tracked_objects(iteration= 0, time_horizon=processor.past_time_horizon, num_samples = int( 10 * processor.past_time_horizon)))\n",
    "track_objs = neighbor_past[0].tracked_objects # center, track_token\n",
    "print(len(neighbor_past))\n",
    "\n",
    "neighbor_future = list(processor.scenario.get_future_tracked_objects(iteration= 0, time_horizon=future_scenario_horizon, num_samples =int( 10 * future_scenario_horizon)))\n",
    "neighbor_states = neighbor_past +[neighbors]+ neighbor_future\n",
    "print(len(neighbor_states))\n",
    "\n",
    "padded_neighbors= process_tracked_objects(neighbor_states, 201, filter_agent_frames_threshold)\n",
    "print(len(padded_neighbors))\n",
    "padded_neighbors_array = np.array(list(padded_neighbors.values()))\n",
    "print(type(padded_neighbors_array))\n",
    "print(padded_neighbors_array.shape)\n",
    "# padded_neighbors_array = np.swapaxes(padded_neighbors_array, 0, 1)\n",
    "local_padded_neighbor_states_array = []\n",
    "for i in range(len(padded_neighbors_array)):\n",
    "  local_padded_neighbor_states_array.append(np.array(convert_absolute_quantities_to_relative(padded_neighbors_array[i, :, :], processor.anchor_ego_state, 'pos_heading_only')))\n",
    "# local_padded_neighbor_states_array = np.array(convert_absolute_quantities_to_relative(padded_neighbors_array[0, :, :], processor.anchor_ego_state, 'pos_heading_only'))\n",
    "local_padded_neighbor_states_array = np.array(local_padded_neighbor_states_array)\n",
    "print(type(local_padded_neighbor_states_array))\n",
    "local_padded_neighbor_agents_past_array = local_padded_neighbor_states_array[:, :20+ 1, :]\n",
    "\n",
    "# print(horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from nuplan.planning.training.preprocessing.utils.agents_preprocessing import EgoInternalIndex, AgentInternalIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_planner.data_process.data_processor import DataProcessor\n",
    "print(len(neighbor_states))\n",
    "\n",
    "# padded_neighbor_states = list(padded_neighbors)\n",
    "print(len(padded_neighbors))\n",
    "object_traj_list = []\n",
    "# for key, value in padded_neighbors.items():\n",
    "#   object_traj_list.append(value)\n",
    "\n",
    "local_object_traj_list = []\n",
    "# print(len(padded_neighbor_states))\n",
    "\n",
    "# local_object_traj_list = np.array(convert_absolute_quantities_to_relative(object_traj_list, anchor_ego_state, 'pos_heading_only'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tracked_objects(tracked_objects_in_all_frames, full_time_horizon: int):\n",
    "\n",
    "  # 创建一个字典来存储所有障碍物的轨迹\n",
    "  object_trajectories = {}\n",
    "\n",
    "  # 遍历每一帧的 tracked_objects\n",
    "  for frame_idx, detection_frame in enumerate(tracked_objects_in_all_frames):\n",
    "      for obj in detection_frame.tracked_objects:\n",
    "          obj_token = obj.track_token  # 获取障碍物 ID\n",
    "          obj_state = np.array([obj.center.x, obj.center.y, obj.center.heading])  # 获取障碍物位置 (x, y)\n",
    "\n",
    "          # 如果该障碍物 ID 还未存储，则初始化\n",
    "          if obj_token not in object_trajectories:\n",
    "              object_trajectories[obj_token] = []\n",
    "\n",
    "          # 记录该障碍物在该帧的位置信息\n",
    "          object_trajectories[obj_token].append(obj_state)\n",
    "\n",
    "  # 统一轨迹长度，填充 NaN\n",
    "  for obj_token, traj in object_trajectories.items():\n",
    "      traj = np.array(traj)  # 转换为 NumPy 数组，形状为 (N, 2)\n",
    "      num_frames = traj.shape[0]\n",
    "\n",
    "      if num_frames < full_time_horizon:\n",
    "          # 如果轨迹长度小于 V，则用 NaN 填充\n",
    "          pad_size = full_time_horizon - num_frames\n",
    "          pad_array = np.full((pad_size, 3), np.nan)  # 生成 (pad_size, 2) 的 NaN 数组\n",
    "          object_trajectories[obj_token] = np.vstack([traj, pad_array])  # 拼接填充\n",
    "      else:\n",
    "          # 如果轨迹长度大于 V，则裁剪\n",
    "          object_trajectories[obj_token] = traj[:full_time_horizon]\n",
    "\n",
    "  return object_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_planner.data_process.utils import (\n",
    "convert_absolute_quantities_to_relative,\n",
    "_global_state_se2_array_to_local,\n",
    "_global_velocity_to_local)\n",
    "\n",
    "\n",
    "from nuplan.planning.training.preprocessing.utils.agents_preprocessing import EgoInternalIndex, AgentInternalIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_pred_horizon =5.0\n",
    "filter_\n",
    "object_traj = process_tracked_objects(neighbor_states, int(1 + (processor.past_time_horizon + future_scenario_horizon + default_pred_horizon)*10), )\n",
    "# object_traj_tensor = torch.full(object_traj.__len__)\n",
    "print(type(object_traj))\n",
    "# for key, value in object_traj.items():\n",
    "#   object_traj_list.append(value)\n",
    "\n",
    "print(len(object_traj))\n",
    "local_object_traj_list = []\n",
    "print(processor.scenario.initial_ego_state)\n",
    "print(EgoInternalIndex.x())\n",
    "ego_state = processor.scenario.initial_ego_state\n",
    "\n",
    "anchor_ego_state = np.array([ego_state.rear_axle.x, ego_state.rear_axle.y, ego_state.rear_axle.heading], dtype=np.float64)\n",
    "# local_coords_agent_states.append(convert_absolute_quantities_to_relative(object_traj_list, processor.anchor_ego_state, 'pos_heading_only'))\n",
    "local_object_traj_list = convert_absolute_quantities_to_relative(object_traj_list, anchor_ego_state, 'pos_heading_only')\n",
    "print(len(local_object_traj_list))\n",
    "print(local_object_traj_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(local_object_traj_list))\n",
    "ar = np.array(local_object_traj_list)\n",
    "print(ar.shape)\n",
    "local_padded_neighbor_agents_past = ar[:, :21, ...]\n",
    "print(local_padded_neighbor_agents_past.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ego_state(ego_states_in_all_frames, full_time_horizon: int):\n",
    "\n",
    "  output = torch.zeros((len(ego_states_in_all_frames), 3), dtype=torch.float32)\n",
    "  for i in range(0, len(ego_states_in_all_frames), 1):\n",
    "    output[i, 0] = ego_states_in_all_frames[i].rear_axle.x\n",
    "    output[i, 1] = ego_states_in_all_frames[i].rear_axle.y\n",
    "    output[i, 2] = ego_states_in_all_frames[i].rear_axle.heading\n",
    "\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_traj = list(future_ego_states) + sampled_past_ego_states\n",
    "print(len(ego_traj))\n",
    "ego_traj = process_ego_state(ego_traj, int(1 + (processor.past_time_horizon + future_scenario_horizon + default_pred_horizon)*10) )\n",
    "print(ego_traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_planner.data_process.utils import (\n",
    "convert_absolute_quantities_to_relative,\n",
    "_global_state_se2_array_to_local,\n",
    "_global_velocity_to_local)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 处理时间格式\n",
    "\n",
    "save_path = \"/cailiu2/Diffusion-Planner/data/processed\"\n",
    "# 生成文件名\n",
    "file_name = f\"{processor.scenario.log_name}_{processor.scenario.token}_{processor.scenario.scenario_type}.pt\"\n",
    "\n",
    "# 确保文件名合法（移除特殊字符）\n",
    "file_name = file_name.replace(\":\", \"_\").replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "# 指定存放路径\n",
    "os.makedirs(save_path, exist_ok=True)  # 确保目录存在\n",
    "\n",
    "# 完整文件路径\n",
    "file_path = os.path.join(save_path, file_name)\n",
    "torch.save(data, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())  # 如果为 True，表示有 CUDA 可用；如果为 False，则没有 CUDA 可用\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_planner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
